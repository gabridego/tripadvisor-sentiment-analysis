{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data science lab: process and methods #\n",
    "Winter project, A.A. 2019/2020  \n",
    "Gabriele Degola, s273479"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import re\n",
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import spacy\n",
    "from spacy_langdetect import LanguageDetector\n",
    "from wordcloud import WordCloud\n",
    "from googletrans import Translator\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "import nltk\n",
    "from collections import Counter\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "nltk.download('stopwords')\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import LinearSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv(r\"./data/development.csv\")\n",
    "print(dataset.shape)\n",
    "test = pd.read_csv(r\"./data/evaluation.csv\")\n",
    "print(test.shape)\n",
    "other = pd.read_csv(r\"./data/external.csv\") # additional reviews are loaded\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data exploration ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset['class'].unique())\n",
    "dataset['class'].hist(bins=3) # plot labels distribution\n",
    "plt.show()\n",
    "dataset['class'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WordCloud generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordList = []\n",
    "sw = stopwords.words('italian')\n",
    "sw.append('molto')\n",
    "for rev in dataset['text'].append(test['text'], ignore_index=True):\n",
    "    rev = rev.lower()\n",
    "    rev = re.sub('[^a-zÃ¨Ã Ã¬Ã¹Ã²Ã©]', ' ', rev)\n",
    "    for word in rev.split():\n",
    "        if len(word) > 3 and word not in sw:\n",
    "            wordList.append(word)\n",
    "fdist = FreqDist(wordList)\n",
    "\n",
    "wc = WordCloud(width=1200, height=800, background_color='white')\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.imshow(wc.generate_from_frequencies(dict(fdist.most_common(200))), interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replace three dots with ellipsis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['text'] = dataset['text'].apply(lambda x: x.replace('...','â€¦'))\n",
    "test['text'] = test['text'].apply(lambda x: x.replace('...','â€¦'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analyse most frequent characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_most_occ_char(input_list):\n",
    "    wc = Counter(\"\".join(input_list))\n",
    "    print(wc)\n",
    "\n",
    "print(\"positive training reviews:\", end=' ')\n",
    "find_most_occ_char(dataset.loc[dataset['class']=='pos', 'text'])\n",
    "print(\"negative training reviews:\", end=' ')\n",
    "find_most_occ_char(dataset.loc[dataset['class']=='neg', 'text'])\n",
    "print(\"test reviews:\", end=' ')\n",
    "find_most_occ_char(test['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot punctuation distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_occurrencies(chars, string):\n",
    "    tot = 0\n",
    "    for char in chars:\n",
    "        tot += string.count(char)\n",
    "    return tot\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "width = 0.4\n",
    "x = np.arange(3)\n",
    "y = []\n",
    "tmp = []\n",
    "for rev in dataset.loc[dataset['class'] == 'pos', 'text']:\n",
    "    tmp.append(count_occurrencies(['â€¦'],rev))\n",
    "y.append(np.mean(tmp))\n",
    "tmp = []\n",
    "for rev in dataset.loc[dataset['class'] == 'pos', 'text']:\n",
    "    tmp.append(count_occurrencies(['!'],rev))\n",
    "y.append(np.mean(tmp))\n",
    "tmp = []\n",
    "for rev in dataset.loc[dataset['class'] == 'pos', 'text']:\n",
    "    tmp.append(count_occurrencies(['?'],rev))\n",
    "y.append(np.mean(tmp))\n",
    "ax.bar(x-width/2, y, width=width, label='pos')\n",
    "y = []\n",
    "tmp = []\n",
    "for rev in dataset.loc[dataset['class'] == 'neg', 'text']:\n",
    "    tmp.append(count_occurrencies(['â€¦'],rev))\n",
    "y.append(np.mean(tmp))\n",
    "tmp = []\n",
    "for rev in dataset.loc[dataset['class'] == 'neg', 'text']:\n",
    "    tmp.append(count_occurrencies(['!'],rev))\n",
    "y.append(np.mean(tmp))\n",
    "tmp = []\n",
    "for rev in dataset.loc[dataset['class'] == 'neg', 'text']:\n",
    "    tmp.append(count_occurrencies(['?'],rev))\n",
    "y.append(np.mean(tmp))\n",
    "ax.bar(x+width/2, y, width=width, label='neg')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(['â€¦','!','?'])\n",
    "plt.title(\"Average occurencies per review\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find and translate non italian reviews with spacy's language detector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('it_core_news_sm', disable=['ner'])\n",
    "nlp.add_pipe(LanguageDetector(), name='language_detector', last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrong = []\n",
    "for i,rev in enumerate(dataset['text']):\n",
    "    if nlp(rev)._.language['language'] != 'it':\n",
    "        wrong.append(i)\n",
    "        \n",
    "wrong_test = []\n",
    "for i,rev in enumerate(test['text']):\n",
    "    if nlp(rev)._.language['language'] != 'it':\n",
    "        wrong_test.append(i)\n",
    "        \n",
    "dataset.iloc[wrong]['text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In development set, if a review is totally not in italian it is removed, in the evaluation set is translated. If multilingual only italian part is kept."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rev_translate(df, ind, label, action):\n",
    "    for i in ind:\n",
    "        it = [str(x) for x in nlp(df.loc[i, label]).sents if x._.language['language'] == 'it'] # italian sentences\n",
    "        if not it:\n",
    "            if action == 'translate':\n",
    "                trans = Translator()\n",
    "                df.loc[i, label] = trans.translate(df.loc[i, label], src=nlp(df.loc[i, label])._.language['language'], dest='it').text\n",
    "            else if action == 'remove':\n",
    "                df.drop(i, inplace=True)\n",
    "        else:\n",
    "            df.loc[i, label] = \" \".join(it)\n",
    "\n",
    "rev_translate(dataset, wrong, 'text', 'remove')\n",
    "rev_translate(test, wrong_test, 'text', 'translate')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clean reviews, lowercase, remove useless punctuation and keep emojis. Emojis and !?â€¦ are treated as separate tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_spaces(x):\n",
    "    for char in ['?','!','â€¦']:\n",
    "        x = x.replace(char,' ' + char + ' ')\n",
    "    return x\n",
    "\n",
    "dataset['cleaned_text'] = dataset['text'].apply(lambda x: x.lower())\n",
    "dataset['cleaned_text'] = dataset['cleaned_text'].apply(lambda x: re.sub(\"[^a-zÃ¨Ã Ã¬Ã¹Ã²Ã©?!â€¦ğŸ‘ğŸ‘ğŸ”ğŸ˜Šâ¤â˜ºğŸ˜‰ğŸ˜±ğŸ˜³ğŸ˜˜ğŸ˜ğŸ™ˆğŸ¤”ğŸ˜¡ğŸ‘ğŸ˜…ğŸ˜ ğŸ˜‚]\", \" \", x))\n",
    "dataset['cleaned_text'] = dataset['cleaned_text'].apply(add_spaces)\n",
    "test['cleaned_text'] = test['text'].apply(lambda x: x.lower())\n",
    "test['cleaned_text'] = test['cleaned_text'].apply(lambda x: re.sub(\"[^a-zÃ¨Ã Ã¬Ã¹Ã²Ã©?!â€¦ğŸ‘ğŸ‘ğŸ”ğŸ˜Šâ¤â˜ºğŸ˜‰ğŸ˜±ğŸ˜³ğŸ˜˜ğŸ˜ğŸ™ˆğŸ¤”ğŸ˜¡ğŸ‘ğŸ˜…ğŸ˜ ğŸ˜‚]\", \" \", x))\n",
    "test['cleaned_text'] = test['cleaned_text'].apply(add_spaces)\n",
    "dataset['size'] = dataset['cleaned_text'].apply(lambda x: len(x.split()))\n",
    "test['size'] = test['cleaned_text'].apply(lambda x: len(x.split()))\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_most_occ_char(dataset.loc[dataset['class']=='pos', 'cleaned_text'])\n",
    "find_most_occ_char(dataset.loc[dataset['class']=='neg', 'cleaned_text'])\n",
    "find_most_occ_char(test['cleaned_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analyse reviews' length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['size'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset.loc[(dataset['size'] < 15), 'text'])\n",
    "print(dataset.loc[(dataset['size'] > 1000), 'text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.loc[(dataset['class'] == 'pos'), 'size'].plot(kind='hist', bins=500, figsize=(10,5), title=\"Reviews length distribution\", label='pos')\n",
    "test['size'].plot(kind='hist', bins=500, label='eval')\n",
    "dataset.loc[(dataset['class'] == 'neg'), 'size'].plot(kind='hist', bins=500, label='neg')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check correlation with proper names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = pd.read_csv(r'nomi.csv', usecols=['Name'], squeeze=True)\n",
    "names = pd.Series(names.apply(lambda x: x.lower()).unique())\n",
    "names.isin(['mia']).sum()\n",
    "names = names[names != 'mia']\n",
    "names.isin([\"nicolo'\"]).sum()\n",
    "names = names.apply(lambda x: x.replace(\"'\",''))\n",
    "names.isin([\"nicolo'\"]).sum()\n",
    "\n",
    "def has_names(x):\n",
    "    for name in names:\n",
    "        if name in x.split():\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "dataset['names'] = dataset['cleaned_text'].apply(has_names)\n",
    "dataset[dataset['names'] == True]['class'].hist(bins=3)\n",
    "plt.show()\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preprocessing ###\n",
    "Convert text to bag-of-words model with tf-idf weighting scheme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StemTokenizer(object):\n",
    "    def __init__(self):\n",
    "        self.stemmer = SnowballStemmer('italian')\n",
    "    def __call__(self, document):\n",
    "        lemmas = []\n",
    "        for t in word_tokenize(document):\n",
    "            t = self.stemmer.stem(t)\n",
    "            t = t.strip()\n",
    "            if len(t) > 2 or t in ['?','!','â€¦','ğŸ‘','ğŸ‘','ğŸ”','ğŸ˜Š','â¤','â˜º','ğŸ˜‰','ğŸ˜±','ğŸ˜³','ğŸ˜˜','ğŸ˜','ğŸ™ˆ','ğŸ¤”','ğŸ˜¡','ğŸ‘','ğŸ˜…','ğŸ˜ ','ğŸ˜‚']:\n",
    "                lemmas.append(t)\n",
    "        return lemmas\n",
    "    \n",
    "# process stopwords\n",
    "sw = stopwords.words('italian')\n",
    "sw.remove('non')\n",
    "stemmer = SnowballStemmer('italian')\n",
    "new_sw = set()\n",
    "for t in sw:\n",
    "    new_sw.add(stemmer.stem(t))\n",
    "\n",
    "tokenizer = StemTokenizer()\n",
    "vect = TfidfVectorizer(tokenizer=tokenizer, max_df=0.6, min_df=4, ngram_range=(1,3), stop_words=new_sw)\n",
    "data = dataset['cleaned_text']\n",
    "data = data.append(test['cleaned_text'], ignore_index=True)\n",
    "vect.fit(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tfidf.transform(dataset['cleaned_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try dimensionality reduction, low performances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# svd = TruncatedSVD(n_components=2000)\n",
    "# X_svd = svd.fit_transform(X)\n",
    "# print(svd.explained_variance_ratio_.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algorithm choice ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multinomial Naive Bayes classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = MultinomialNB()\n",
    "f1 = cross_val_score(clf, X, dataset['class'], cv=5, scoring='f1_weighted')\n",
    "print('MultinomialNB:')\n",
    "print('F1 scores:')\n",
    "print(f1)\n",
    "print(f1.mean())\n",
    "\n",
    "y_pred_nb = cross_val_predict(clf, X, dataset['class'], cv=5)\n",
    "print('Accuracy:')\n",
    "print(accuracy_score(dataset['class'], y_pred_nb))\n",
    "print('Precision recall fscore support')\n",
    "print(precision_recall_fscore_support(dataset['class'], y_pred_nb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Support Vector Machines classifier with linear kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LinearSVC(max_iter=2000)\n",
    "f1 = cross_val_score(clf, X, dataset['class'], cv=5, scoring='f1_weighted')\n",
    "print('LinearSVC:')\n",
    "print('F1 scores:')\n",
    "print(f1)\n",
    "print(f1.mean())\n",
    "\n",
    "y_pred_svc = cross_val_predict(clf, X, dataset['class'], cv=5)\n",
    "print('Accuracy:')\n",
    "print(accuracy_score(dataset['class'], y_pred_svc))\n",
    "print('Precision recall fscore support')\n",
    "print(precision_recall_fscore_support(dataset['class'], y_pred_svc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forest classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = RandomForestClassifier(n_estimators=100)\n",
    "f1 = cross_val_score(clf, X, dataset['class'], cv=5, scoring='f1_weighted')\n",
    "print('RandomForestClassifier:')\n",
    "print('F1 scores:')\n",
    "print(f1)\n",
    "print(f1.mean())\n",
    "\n",
    "y_pred_rf = cross_val_predict(clf, X, dataset['class'], cv=5)\n",
    "print('Accuracy:')\n",
    "print(accuracy_score(dataset['class'], y_pred_rf))\n",
    "print('Precision recall fscore support')\n",
    "print(precision_recall_fscore_support(dataset['class'], y_pred_rf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot ROC curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapp = {'pos':1, 'neg':0}\n",
    "class_bin = dataset['class'].replace(mapp)\n",
    "\n",
    "fpr_svc, tpr_svc, _ = roc_curve(class_bin, pd.Series(y_pred_svc).replace(mapp))\n",
    "fpr_nb, tpr_nb, _ = roc_curve(class_bin, pd.Series(y_pred_nb).replace(mapp))\n",
    "fpr_rf, tpr_rf, _ = roc_curve(class_bin, pd.Series(y_pred_rf).replace(mapp))\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.title('ROC curve comparison')\n",
    "plt.plot([0, 1], [0, 1],linestyle='--')\n",
    "plt.plot(fpr_svc, tpr_svc, label='Linear SVC')\n",
    "plt.plot(fpr_nb, tpr_nb, label='Multinomial NB')\n",
    "plt.plot(fpr_rf, tpr_rf, label='Random Forest')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuning and validation ###\n",
    "Grid search to find best hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LinearSVC(max_iter=2000)\n",
    "grid = {'dual':[True,False], 'tol':[1e-3,1e-4,1e-5], 'C':[0.1,1,10,25,50,100], 'fit_intercept':[True,False], 'intercept_scaling':[0.1,1,10], 'class_weight':[None,'balanced']}\n",
    "gridsearch = GridSearchCV(clf, grid, scoring='f1_weighted', cv=3, verbose=2, error_score='raise')\n",
    "gridsearch.fit(X, dataset['class'])\n",
    "\n",
    "print(gridsearch.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = gridsearch.best_estimator_\n",
    "f1 = cross_val_score(clf, X, dataset['class'], cv=5, scoring='f1_weighted')\n",
    "print('F1 scores for the best estimator:')\n",
    "print(f1)\n",
    "print(f1.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_val = cross_val_predict(clf, X, dataset['class'], cv=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot confusion matrix and normalized confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(1,2, figsize=(14,5))\n",
    "\n",
    "# build the confusion matrix\n",
    "conf_mat = confusion_matrix(class_bin, pd.Series(y_pred_val).replace(mapp))\n",
    "\n",
    "# plot the result\n",
    "conf_mat_df = pd.DataFrame(conf_mat, index = ['neg','pos'], columns = ['neg','pos'])\n",
    "conf_mat_df.index.name = 'Actual'\n",
    "conf_mat_df.columns.name = 'Predicted'\n",
    "sns.heatmap(conf_mat_df, annot=True, cmap='GnBu', annot_kws={\"size\": 16}, fmt='g', cbar=True, ax=ax[0])\n",
    "ax[0].set_title('Confusion matrix')\n",
    "\n",
    "# count positive and negative labels in the development set\n",
    "y_pos = class_bin.value_counts()[1]\n",
    "y_neg = class_bin.value_counts()[0]\n",
    "\n",
    "# plot normalized confusion matrix with recall on the diagonal\n",
    "norm_conf_mat = conf_mat * np.array([[1.0 / y_neg,1.0/y_neg],[1.0/y_pos,1.0/y_pos]])\n",
    "norm_conf_mat_df = pd.DataFrame(norm_conf_mat, index = ['neg','pos'], columns = ['neg','pos'])\n",
    "norm_conf_mat_df.index.name = 'Actual'\n",
    "norm_conf_mat_df.columns.name = 'Predicted'\n",
    "sns.heatmap(norm_conf_mat_df, annot=True, cmap='GnBu', annot_kws={\"size\": 16}, fmt='g', cbar=True, ax=ax[1])\n",
    "ax[1].set_title('Normalized confusion matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation set prediction ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = vect.transform(test['cleaned_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = gridsearch.best_estimator_\n",
    "clf.fit(X, dataset['class'])\n",
    "y_pred = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(y_pred).hist(bins=3) # predicted labels distribution\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Export the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"out.csv\", mode='w', newline=\"\", encoding='UTF-8') as f:\n",
    "    writer = csv.writer(f, delimiter=',')\n",
    "    writer.writerow(['Id', 'Predicted'])\n",
    "    for j,el in enumerate(y_pred):\n",
    "        writer.writerow([j,el])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
